{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 201 Programming Assignment 2\n",
    "## Naive Bayes Spam Filter\n",
    "\n",
    "Submitted by: \n",
    "Jan Lendl R. Uy, 2019-00312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File constants\n",
    "DATASET_PATH_STRING = \"trec06p-ai201/data\"\n",
    "FOLDER_COUNT = 127\n",
    "FILE_COUNT = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the contents of the TREC06 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "df_labels = pd.read_csv(\"trec06p-ai201/labels\", header=None)\n",
    "labels_in_list = df_labels.values.tolist()\n",
    "\n",
    "for label in labels_in_list:\n",
    "    label_in_str = str(label).rsplit(\" \")\n",
    "    labels.append(label_in_str[0][2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more files left to read!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for i in range(FOLDER_COUNT):\n",
    "    folder_count = str(i)\n",
    "    # Convert folder digit directory to a 3-character string\n",
    "    if len(folder_count) < 3:\n",
    "        folder_count = \"0\" * (3-len(folder_count)) + folder_count\n",
    "    for j in range(FILE_COUNT):\n",
    "        file_count = str(j)\n",
    "        # Convert digit filename to a 3-character string\n",
    "        if len(file_count) < 3:\n",
    "            file_count = \"0\" * (3-len(file_count)) + file_count\n",
    "        directory_path = f\"{DATASET_PATH_STRING}/{folder_count}/{file_count}\"\n",
    "        try:\n",
    "            with open(file=directory_path, \n",
    "                    mode=\"r\", \n",
    "                    encoding=\"utf-8\", \n",
    "                    errors=\"replace\") as file:\n",
    "                content = file.read().replace(\"ï¿½\", \"\")\n",
    "                documents.append(content)\n",
    "        except:\n",
    "            print(f\"No more files left to read!\")\n",
    "            break\n",
    "        # print(f\"{directory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37822\n",
      "37822\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def custom_train_test_split(X, Y, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Splits the documents and their corresponding labels into training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of documents.\n",
    "    - labels: List of labels corresponding to the documents.\n",
    "    - test_size: Proportion of the dataset to include in the test split (float or int).\n",
    "    \n",
    "    Returns:\n",
    "    - train_docs: List of training documents.\n",
    "    - test_docs: List of testing documents.\n",
    "    - train_labels: List of labels for the training documents.\n",
    "    - test_labels: List of labels for the testing documents.\n",
    "    \"\"\"\n",
    "    # Pair each document with its label\n",
    "    paired = list(zip(X, Y))\n",
    "    \n",
    "    # Shuffle the paired documents and labels\n",
    "    random.shuffle(paired)\n",
    "    \n",
    "    # Calculate the number of samples in the test set\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(test_size * len(X))\n",
    "    \n",
    "    # Split the paired list into training and testing sets\n",
    "    train_pairs = paired[:-test_size]\n",
    "    test_pairs = paired[-test_size:]\n",
    "    \n",
    "    # Unzip the pairs back into separate lists\n",
    "    train_docs, train_labels = zip(*train_pairs)\n",
    "    test_docs, test_labels = zip(*test_pairs)\n",
    "    \n",
    "    return list(train_docs), list(test_docs), list(train_labels), list(test_labels)\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'documents' and 'labels' are defined\n",
    "train_docs, test_docs, train_labels, test_labels = custom_train_test_split(documents, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = custom_train_test_split(documents, labels, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the Naive Bayes Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, lambda_value=1):\n",
    "        self.lambda_value = 1  # Laplace smoothing factor\n",
    "        self.vocabulary_size = None\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.vocabulary = set()\n",
    "        self.word_counts = None\n",
    "        self.spam_count = None\n",
    "        self.ham_count = None\n",
    "\n",
    "        # Initialize log likelihoods\n",
    "        self.log_likelihoods = None\n",
    "        \n",
    "        self.log_prior_spam, self.log_prior_ham = None, None\n",
    "        \n",
    "    def build_vocabulary(self, X_train, Y_train):\n",
    "        # Assuming 'documents' is a list of strings (each string is a document),\n",
    "        # and 'labels' is a list of labels ('spam' or 'ham') corresponding to each document.\n",
    "        \n",
    "        self.word_counts = {\"spam\": {}, \"ham\": {}}\n",
    "        self.spam_count = 0\n",
    "        self.ham_count = 0\n",
    "\n",
    "        # Preprocess documents and build vocabulary\n",
    "        for document, label in zip(X_train, Y_train):\n",
    "            # Tokenize the document\n",
    "            words = re.findall(\"[a-zA-Z]+\", document)\n",
    "            unique_words = set(words)\n",
    "            \n",
    "            # Update vocabulary\n",
    "            self.vocabulary.update(unique_words)\n",
    "            \n",
    "            # Count word statistics\n",
    "            for word in unique_words:\n",
    "                if word not in self.word_counts[label]:\n",
    "                    self.word_counts[label][word] = 0\n",
    "                self.word_counts[label][word] += 1\n",
    "            \n",
    "            # Count documents in each class\n",
    "            if label == \"spam\":\n",
    "                self.spam_count += 1\n",
    "            else:\n",
    "                self.ham_count += 1\n",
    "\n",
    "        # Calculate prior probabilities\n",
    "        total_documents = self.spam_count + self.ham_count\n",
    "        prior_spam = self.spam_count / total_documents\n",
    "        prior_ham = self.ham_count / total_documents\n",
    "        \n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"Prior probability of spam: {prior_spam}\")\n",
    "        print(f\"Prior probability of ham: {prior_ham}\")\n",
    "        \n",
    "        # Initialize log likelihoods\n",
    "        self.log_likelihoods = {\n",
    "            \"spam\": {word: 0 for word in self.vocabulary},\n",
    "            \"ham\": {word: 0 for word in self.vocabulary},\n",
    "        }\n",
    "        \n",
    "        return np.log(prior_spam), np.log(prior_ham)\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        # Calculate log priors\n",
    "        self.log_prior_spam, self.log_prior_ham = self.build_vocabulary(X_train, Y_train)\n",
    "        \n",
    "        # Calculate log likelihoods for each word\n",
    "        for word in self.vocabulary:\n",
    "            # Calculate the likelihood of word given spam\n",
    "            spam_word_count = self.word_counts[\"spam\"].get(word, 0)\n",
    "            spam_likelihood = (spam_word_count + self.lambda_value) / (self.spam_count + self.lambda_value * self.vocabulary_size)\n",
    "            self.log_likelihoods[\"spam\"][word] = np.log(spam_likelihood)\n",
    "            \n",
    "            # Calculate the likelihood of word given ham\n",
    "            ham_word_count = self.word_counts[\"ham\"].get(word, 0)\n",
    "            ham_likelihood = (ham_word_count + self.lambda_value) / (self.ham_count + self.lambda_value * self.vocabulary_size)\n",
    "            self.log_likelihoods[\"ham\"][word] = np.log(ham_likelihood)\n",
    "\n",
    "    def predict(self, X_test):        \n",
    "        predictions = []\n",
    "        \n",
    "        for document in X_test:\n",
    "            words = re.findall(\"[a-zA-Z]+\", document)\n",
    "            spam_score = self.log_prior_spam\n",
    "            ham_score = self.log_prior_ham\n",
    "            for word in words:\n",
    "                if word in self.log_likelihoods[\"spam\"]:\n",
    "                    spam_score += self.log_likelihoods[\"spam\"][word]\n",
    "                if word in self.log_likelihoods[\"ham\"]:\n",
    "                    ham_score += self.log_likelihoods[\"ham\"][word]\n",
    "        \n",
    "            predictions.append(\"spam\" if spam_score > ham_score else \"ham\")\n",
    "            \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(predicted_labels, actual_labels):\n",
    "    true_positive = sum(1 for predicted, actual in zip(predicted_labels, actual_labels) if predicted == actual and actual == \"spam\")\n",
    "    false_positive = sum(1 for predicted, actual in zip(predicted_labels, actual_labels) if predicted == \"spam\" and actual == \"ham\")\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def get_recall(predicted_labels, actual_labels):\n",
    "    true_positive = sum(1 for predicted, actual in zip(predicted_labels, actual_labels) if predicted == actual and actual == \"spam\")\n",
    "    false_negative = sum(1 for predicted, actual in zip(predicted_labels, actual_labels) if predicted == \"ham\" and actual == \"spam\")\n",
    "        \n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2182521\n",
      "Prior probability of spam: 0.6573500528780782\n",
      "Prior probability of ham: 0.34264994712192176\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier(lambda_value=1)\n",
    "classifier.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of the Naive Bayes Classifier: 0.9351701782820098\n",
      "Recall of the Naive Bayes Classifier: 0.9990676611614278\n"
     ]
    }
   ],
   "source": [
    "Y_pred = classifier.predict(X_test)\n",
    "print(f\"Precision of the Naive Bayes Classifier: {get_precision(Y_pred, Y_test)}\")\n",
    "print(f\"Recall of the Naive Bayes Classifier: {get_recall(Y_pred, Y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
