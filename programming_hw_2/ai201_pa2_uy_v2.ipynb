{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 201 Programming Assignment 2\n",
    "## Naive Bayes Spam Filter Implementation\n",
    "\n",
    "Submitted by: \n",
    "Jan Lendl R. Uy, 2019-00312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zstandard in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lendluy/Library/Python/3.11/lib/python/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/lendluy/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lendluy/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install zstandard matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:21:37.945708Z",
     "start_time": "2024-10-01T15:21:37.629546Z"
    }
   },
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import re\n",
    "# import random\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Constants\n",
    "\n",
    "# PATH_TO_TREC06 = \"trec06p-ai201/\"\n",
    "TREC06_TRAIN_SET = \"train_dataset.pkl.zst\"\n",
    "TREC06_TEST_SET = \"test_dataset.pkl.zst\"\n",
    "TOP_N_WORDS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous code for reading and processing the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:21:37.951725Z",
     "start_time": "2024-10-01T15:21:37.946726Z"
    }
   },
   "outputs": [],
   "source": [
    "# def read_labels(base_path):\n",
    "#     # Path to the labels file\n",
    "#     labels_path = os.path.join(base_path, \"labels\")\n",
    "\n",
    "#     labels_dict = {}\n",
    "#     with open(labels_path, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             parts = line.strip().split()\n",
    "#             if len(parts) == 2:\n",
    "#                 label, file_path = parts\n",
    "#                 # Don't include \"..\" which is a prefix in the labels file\n",
    "#                 file_path = file_path[3:]\n",
    "#                 labels_dict[file_path] = label\n",
    "                \n",
    "#     return labels_dict\n",
    "\n",
    "# def tokenize(text):\n",
    "    \n",
    "#     return re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "\n",
    "# def read_documents(base_path, labels_dict):\n",
    "#     X = []  # List to store document contents\n",
    "#     Y = []  # List to store labels\n",
    "    \n",
    "#     # Root directory for documents\n",
    "#     data_dir = os.path.join(base_path, \"data\")\n",
    "\n",
    "#     # Iterate through all subdirectories\n",
    "#     for subdir in os.listdir(data_dir):\n",
    "#         subdir_path = os.path.join(data_dir, subdir)\n",
    "#         if os.path.isdir(subdir_path):\n",
    "#             for file in os.listdir(subdir_path):\n",
    "#                 file_path = os.path.join(subdir_path, file)\n",
    "#                 relative_path = os.path.relpath(file_path, base_path)\n",
    "#                 if relative_path in labels_dict:\n",
    "#                     with open(file_path, \"r\", errors=\"ignore\") as f:\n",
    "#                         content = f.read()\n",
    "                        \n",
    "#                     # Tokenization\n",
    "#                     words = tokenize(content)\n",
    "                    \n",
    "#                     X.append(words)\n",
    "#                     Y.append(1 if labels_dict[relative_path] == \"spam\" else 0)\n",
    "                    \n",
    "#     return X, Y\n",
    "\n",
    "# def read_trec06_corpus(base_path):\n",
    "#     # Get labels\n",
    "#     labels_dict = read_labels(base_path)\n",
    "                \n",
    "#     # Get features\n",
    "#     X, Y = read_documents(base_path, labels_dict)\n",
    "\n",
    "#     return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store features in X and labels in Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:21:46.331583Z",
     "start_time": "2024-10-01T15:21:37.953614Z"
    }
   },
   "outputs": [],
   "source": [
    "# X, Y = read_trec06_corpus(PATH_TO_TREC06)\n",
    "\n",
    "# # Sanity check to determine how much documents were processed\n",
    "# print(f\"Total documents processed: {len(X)}\")\n",
    "# print(f\"Number of spam: {sum(Y)}\")\n",
    "# print(f\"Number of ham: {len(Y) - sum(Y)}\")\n",
    "\n",
    "# # Obtain the first entry for the feature (i.e. document) and its label\n",
    "# # for checking\n",
    "# print(f\"X[:1] = {X[:1]}\")\n",
    "# print(f\"Y[:1] = {Y[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test sets\n",
    "\n",
    "The function below is reminiscent to the `train_test_split()` function of sklearn, which takes in the following parameters:\n",
    "- features X\n",
    "- labels y\n",
    "- test_size in decimal\n",
    "- random_state which sets the seed to randomization of the order of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:21:46.336190Z",
     "start_time": "2024-10-01T15:21:46.333850Z"
    }
   },
   "outputs": [],
   "source": [
    "# def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "#     if random_state is not None:\n",
    "#         random.seed(random_state)\n",
    "    \n",
    "#     # Create list of indices and shuffle it\n",
    "#     indices = list(range(len(X)))\n",
    "#     random.shuffle(indices)\n",
    "    \n",
    "#     # Calculate split point\n",
    "#     split = int(len(X) * (1 - test_size))\n",
    "    \n",
    "#     # Split the data\n",
    "#     train_indices = indices[:split]\n",
    "#     test_indices = indices[split:]\n",
    "    \n",
    "#     X_train = [X[i] for i in train_indices]\n",
    "#     X_test = [X[i] for i in test_indices]\n",
    "#     y_train = [y[i] for i in train_indices]\n",
    "#     y_test = [y[i] for i in test_indices]\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:21:46.417759Z",
     "start_time": "2024-10-01T15:21:46.337956Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=13)\n",
    "\n",
    "# print(f\"Training set size: {len(X_train)}\")\n",
    "# print(f\"Test set size: {len(X_test)}\")\n",
    "# print(f\"Dataset size: {len(X_train) + len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import zstandard as zstd\n",
    "\n",
    "# # Create a compressor with maximum compression level and multi-threading\n",
    "# cctx = zstd.ZstdCompressor(level=22, threads=-1)  # 'threads=-1' uses all available CPU cores\n",
    "\n",
    "# with open('train_dataset.pkl.zst', 'wb') as f:\n",
    "#     with cctx.stream_writer(f) as compressor:\n",
    "#         pickle.dump((X_train, y_train), compressor)\n",
    "        \n",
    "# with open('test_dataset.pkl.zst', 'wb') as f:\n",
    "#     with cctx.stream_writer(f) as compressor:\n",
    "#         pickle.dump((X_test, y_test), compressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test sets from the zst-compressed pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompressor objector for ZSTD-compressed dataset\n",
    "dctx = zstd.ZstdDecompressor()\n",
    "\n",
    "with open(TREC06_TRAIN_SET, \"rb\") as f:\n",
    "    with dctx.stream_reader(f) as decompressor:\n",
    "        X_train, y_train = pickle.load(decompressor)\n",
    "        \n",
    "with open(TREC06_TEST_SET, \"rb\") as f:\n",
    "    with dctx.stream_reader(f) as decompressor:\n",
    "        X_test, y_test = pickle.load(decompressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build the vocabulary\n",
    "Using the documents from `X_train`, create a vocabulary of unique words from the training set. After doing so, compute the class prior probability of a ham and a spam document. This will be called later by the Naive Bayes Classifier implementation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:21:46.418089Z",
     "start_time": "2024-10-01T15:21:46.382700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a vocabulary from the training data\n",
    "def create_vocabulary(X_train):\n",
    "    vocabulary = set()\n",
    "    for document in X_train:\n",
    "        vocabulary.update(document)\n",
    "    return list(vocabulary)\n",
    "\n",
    "# Obtain the prior probabilities from the training data\n",
    "def compute_prior_probabilities(y_train):\n",
    "    total_docs = len(y_train)\n",
    "    label_counts = Counter(y_train)\n",
    "    priors = {}\n",
    "    for label, count in label_counts.items():\n",
    "        priors[label] = count / total_docs\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 1967600\n",
      "Prior probability of ham: 0.34523135033050045\n",
      "Prior probability of spam: 0.6547686496694995\n"
     ]
    }
   ],
   "source": [
    "V = create_vocabulary(X_train)\n",
    "vocab_size = len(V)\n",
    "\n",
    "# print(f\"V = {V}\")\n",
    "print(f\"vocab_size = {vocab_size}\")\n",
    "\n",
    "priors = compute_prior_probabilities(y_train)\n",
    "\n",
    "print(f\"Prior probability of ham: {priors[0]}\")\n",
    "print(f\"Prior probability of spam: {priors[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, smoothing_factor=1):\n",
    "        # Smoothing parameter\n",
    "        # By default, smoothing value is Laplace's (i.e. 1)\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        \n",
    "        self.vocabulary = None\n",
    "        self.vocab_size = None\n",
    "        self.word_to_index = None\n",
    "        self.priors = None\n",
    "        self.likelihoods = None\n",
    "        self.label_word_counts = None\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.create_vocabulary(X_train)\n",
    "\n",
    "        # Compute prior probabilities\n",
    "        self.compute_prior_probabilities(y_train)\n",
    "\n",
    "        # Compute class conditional likelihoods with lambda smoothing\n",
    "        self.compute_likelihoods(X_train, y_train)\n",
    "\n",
    "    def create_vocabulary(self, X_train):\n",
    "        vocabulary_set = set()\n",
    "        for document in X_train:\n",
    "            vocabulary_set.update(document)\n",
    "        self.vocabulary = list(vocabulary_set)\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        # Map words to integer indices for faster lookup\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "    \n",
    "    def compute_prior_probabilities(self, y_train):\n",
    "        total_docs = len(y_train)\n",
    "        label_counts = Counter(y_train)\n",
    "        self.labels = list(label_counts.keys())\n",
    "        self.priors = {label: label_counts[label] / total_docs for label in self.labels}\n",
    "\n",
    "    def compute_likelihoods(self, X_train, y_train):\n",
    "        # Initialize word counts and total word counts per label/class\n",
    "        word_counts = {label: np.zeros(self.vocab_size) for label in self.labels}\n",
    "        self.label_word_counts = {label: 0 for label in self.labels}\n",
    "\n",
    "        # Count word occurrences per class\n",
    "        for words, label in zip(X_train, y_train):\n",
    "            for word in words:\n",
    "                if word in self.word_to_index:\n",
    "                    idx = self.word_to_index[word]\n",
    "                    word_counts[label][idx] += 1\n",
    "                    self.label_word_counts[label] += 1\n",
    "\n",
    "        # Compute smoothed likelihoods and convert to log probabilities\n",
    "        self.likelihoods = {}\n",
    "        for label in self.labels:\n",
    "            numerator = word_counts[label] + self.smoothing_factor\n",
    "            denominator = self.label_word_counts[label] + (self.vocab_size * self.smoothing_factor)\n",
    "            likelihood = numerator / denominator\n",
    "            self.likelihoods[label] = np.log(likelihood)  # Convert to log likelihoods\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        Y_pred = []\n",
    "        log_priors = {label: np.log(self.priors[label]) for label in self.labels}\n",
    "\n",
    "        for document in X_test:\n",
    "            \n",
    "            label_scores = {}\n",
    "            for label in self.labels:\n",
    "                log_prob = log_priors[label]\n",
    "                for word in document:\n",
    "                    if word in self.word_to_index:\n",
    "                        idx = self.word_to_index[word]\n",
    "                        log_prob += self.likelihoods[label][idx]\n",
    "                    else:\n",
    "                        # Word not found in vocabulary\n",
    "                        # Apply lambda smoothing\n",
    "                        total_words_for_label = self.label_word_counts[label]\n",
    "                        denominator = total_words_for_label + (self.vocab_size * self.smoothing_factor)\n",
    "                        smoothed_prob = np.log(self.smoothing_factor) - np.log(denominator)\n",
    "                        log_prob += smoothed_prob\n",
    "                        \n",
    "                # Don't exponentiate log likelihood\n",
    "                # Underflow will still occur because the log likelihood values < 0\n",
    "                # and abs(log likelihood) are large\n",
    "                label_scores[label] = log_prob\n",
    "                \n",
    "            # Select the label with the highest log probability\n",
    "            predicted_label = max(label_scores, key=label_scores.get)\n",
    "            # print(f\"label_scores = {label_scores}\")\n",
    "            # print(f\"predicted_label = {predicted_label}\")\n",
    "            Y_pred.append(predicted_label)\n",
    "            \n",
    "        return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cq/5p30h6x5741g3nvfdq8t32g40000gn/T/ipykernel_66709/3147207065.py:59: RuntimeWarning: divide by zero encountered in log\n",
      "  self.likelihoods[label] = np.log(likelihood)  # Convert to log likelihoods\n",
      "/var/folders/cq/5p30h6x5741g3nvfdq8t32g40000gn/T/ipykernel_66709/3147207065.py:79: RuntimeWarning: divide by zero encountered in log\n",
      "  smoothed_prob = np.log(self.smoothing_factor) - np.log(denominator)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier with no smoothing factor\n",
    "nbc = NaiveBayesClassifier(smoothing_factor=0)\n",
    "\n",
    "# Train the classifier\n",
    "nbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get the precision and recall of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    true_positive = sum((y_t == 1 and y_p == 1) for y_t, y_p in zip(y_true, y_pred))\n",
    "    false_positive = sum((y_t == 0 and y_p == 1) for y_t, y_p in zip(y_true, y_pred))\n",
    "    false_negative = sum((y_t == 1 and y_p == 0) for y_t, y_p in zip(y_true, y_pred))\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9998385273696109\n",
      "Recall: 0.8172099775636795\n"
     ]
    }
   ],
   "source": [
    "initial_metrics = compute_metrics(y_test, y_pred)\n",
    "print(f\"Precision: {initial_metrics['precision']}\")\n",
    "print(f\"Recall: {initial_metrics['recall']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluate classifier for varying smoothing factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes Classifier for Varying Smoothing Factor:  67%|██████▋   | 4/6 [00:44<00:22, 11.12s/it]"
     ]
    }
   ],
   "source": [
    "lambda_values = [1e-3, 1e-2, 0.1, 0.5, 1, 2]\n",
    "precision_results = []\n",
    "recall_results = []\n",
    "\n",
    "for lambda_value in tqdm(lambda_values, desc=\"Training Naive Bayes Classifier for Varying Smoothing Factor\"):\n",
    "    # Create and train a Naive Bayes Classifier for each smoothing factor\n",
    "    nbc = NaiveBayesClassifier(smoothing_factor=lambda_value)\n",
    "    \n",
    "    # Recreate the generator for each iteration\n",
    "    nbc.fit(X_train, y_train)\n",
    "    \n",
    "    # Recreate the generator for prediction\n",
    "    y_pred = nbc.predict(X_test)\n",
    "    \n",
    "    metrics = compute_metrics(y_test, y_pred)\n",
    "\n",
    "    precision_results.append(metrics[\"precision\"])\n",
    "    recall_results.append(metrics[\"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"precision_results = {precision_results}\")\n",
    "print(f\"recall_results = {recall_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plot the performance of the classifier for varying smoothing factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "\n",
    "plt.plot(lambda_values, precision_results, label=\"Precision\", linestyle=\"-\", marker=\"o\", linewidth=2, markersize=8)\n",
    "plt.plot(lambda_values, recall_results, label=\"Recall\", linestyle=\"--\", marker=\"^\", linewidth=2, markersize=8)\n",
    "\n",
    "plt.title(\"Performance of the Naive Bayes - Spam Filter for Varying Smoothing Factors\", fontsize=14)\n",
    "plt.xlabel(\"Lambda Smoothing Factor\", fontsize=12)\n",
    "plt.ylabel(\"Performance\", fontsize=12)\n",
    "\n",
    "plt.xscale(\"log\")  # Logarithmic scale for x-axis\n",
    "\n",
    "# Format x-ticks in decimal notation\n",
    "def decimal_formatter(x, pos):\n",
    "    return f\"{x:.3f}\" if x < 0.1 else f\"{x:.2f}\"\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(decimal_formatter))\n",
    "\n",
    "plt.xlim(min(lambda_values), max(lambda_values))  # Adjust x-axis limits\n",
    "\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)  # Adding grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Based on the plot above, a lower smoothing factor results in the best performance in terms of precision and recall. More specifically, a lambda smoothing value of $0.001$ maximizes both precision and recall on the test set. It can be inferred from the decreasing trend in the plot that choosing a large smoothing factor results in an underfitted classifier due to the degradation in performance. Thus, the best smoothing value is $0.001$. Additionally, it can be observed that the precision is greater than the recall of the classifier for $\\lambda \\leq 0.03$ while the opposite is true for $\\lambda > 0.03$. This presents the fundamental trade-off between precision and recall, wherein an increase in precision results in a decrease in recall and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the Top 200 Words Ranked Based on Mutual Information\n",
    "Instead of frequency, mutual information score is computed for each word in the vocabulary, and this is used to rank words in the training set. Mutual information is given by:\n",
    "\n",
    "$$\n",
    "MI(X, \\omega) = \\Sigma_{i =1}^d \\Sigma_{\\omega \\in \\Omega} P(x_i, \\omega) \\log \\frac{P(x_i, \\omega)}{P(x_i) P(\\omega)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute MI component\n",
    "def mi_component(N, n_ij, n_i_, n_j):\n",
    "    if n_ij == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (n_ij / N) * np.log2((N * n_ij) / (n_i_ * n_j))\n",
    "\n",
    "def compute_mutual_information(X_train, y_train):\n",
    "    # Initialize counts\n",
    "    N = len(X_train)  # Total number of documents\n",
    "    word_counts = defaultdict(lambda: {0: 0, 1: 0})\n",
    "    label_counts = Counter(y_train)\n",
    "        \n",
    "    # Build word-class contingency tables (Optimized)\n",
    "    for words, label in zip(X_train, y_train):\n",
    "        unique_words = set(words)\n",
    "        for word in unique_words:\n",
    "            word_counts[word][label] += 1\n",
    "        \n",
    "    # Compute MI for each word\n",
    "    mi_scores = {}\n",
    "    for word in word_counts:\n",
    "        # Contingency table values\n",
    "        N11 = word_counts[word][1]  # Word present, spam label\n",
    "        N01 = word_counts[word][0]  # Word present, ham label\n",
    "        N1_ = N11 + N01             # Word present\n",
    "        N10 = label_counts[1] - N11 # Word absent, spam label\n",
    "        N00 = label_counts[0] - N01 # Word absent, ham label\n",
    "        N0_ = N10 + N00             # Word absent\n",
    "        \n",
    "        # Total counts for classes\n",
    "        N_1 = label_counts[1]\n",
    "        N_0 = label_counts[0]\n",
    "        \n",
    "        # Mutual Information components\n",
    "        mi = 0\n",
    "        \n",
    "        # Compute MI components\n",
    "        mi += mi_component(N, N11, N1_, N_1)\n",
    "        mi += mi_component(N, N01, N1_, N_0)\n",
    "        mi += mi_component(N, N10, N0_, N_1)\n",
    "        mi += mi_component(N, N00, N0_, N_0)\n",
    "        \n",
    "        mi_scores[word] = mi\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MI scores\n",
    "mi_scores = compute_mutual_information(X_train, y_train)\n",
    "\n",
    "# Sort words by MI score in descending order\n",
    "sorted_words = sorted(mi_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Get the top 200 words\n",
    "i = 1\n",
    "print(\"Top 200 Words Ranked by Mutual Information\")\n",
    "for word, mi_score in sorted_words[:TOP_N_WORDS]:\n",
    "    print(f\"{i}. {word}, Mutual Information: {mi_score}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the classifier without the top 200 words\n",
    "To retrain the classifier without the top 200 words ranked by highest mutual information, remove these words from `X_train` then instantiate and train a new classifier on the reduced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 200 words\n",
    "top_200_words = set(word for word, _ in sorted_words[:TOP_N_WORDS])\n",
    "\n",
    "def remove_top_words(documents, top_words):\n",
    "    return [[word for word in doc if word not in top_words] for doc in documents]\n",
    "\n",
    "# Remove the top 200 words from X_train so that it does not show up in the vocabulary\n",
    "X_train_reduced = remove_top_words(X_train, top_200_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize a Naive Bayes classifier with the best smoothing factor\n",
    "nbc = NaiveBayesClassifier(smoothing_factor=1e-3)\n",
    "\n",
    "# Train the classifier\n",
    "nbc.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = nbc.predict(X_test)\n",
    "\n",
    "reduced_train_set_metrics = compute_metrics(y_test, y_pred)\n",
    "print(f\"Precision: {reduced_train_set_metrics['precision']}\")\n",
    "print(f\"Recall: {reduced_train_set_metrics['recall']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison of precision with and without the top 200 words\n",
    "models = [\"Classifier with Original Vocabulary\", \"Classifier with Top 200 Words Removed\"]\n",
    "original_train_set_metrics = precision_results[0]\n",
    "precision_scores = [original_train_set_metrics, reduced_train_set_metrics[\"precision\"]]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, precision_scores, color=[\"skyblue\", \"salmon\"])\n",
    "plt.title(\"Classifier Precision with and without the Top 200 Words\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim(0.9, 1)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison of recall with and without the top 200 words\n",
    "models = [\"Classifier with Original Vocabulary\", \"Classifier with Top 200 Words Removed\"]\n",
    "original_train_set_metrics = recall_results[0]\n",
    "recall_scores = [original_train_set_metrics, reduced_train_set_metrics[\"recall\"]]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, recall_scores, color=[\"skyblue\", \"salmon\"])\n",
    "plt.title(\"Classifier Recall with and without the Top 200 Words\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.ylim(0.9, 1)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
