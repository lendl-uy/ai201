{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 201 Programming Assignment 3\n",
    "## Balance the Dataset via SMOTE\n",
    "\n",
    "Submitted by: \n",
    "Jan Lendl R. Uy, 2019-00312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import necessary libraries\n",
    "- numpy\n",
    "- imbalanced-learn\n",
    "- matplotlib\n",
    "- tqdm\n",
    "- ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/lendluy/Documents/MEngg AI/AI 201 2nd Take/ai201/.venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/lendluy/Documents/MEngg AI/AI 201 2nd Take/ai201/.venv/lib/python3.11/site-packages (0.12.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/lendluy/Documents/MEngg AI/AI 201 2nd Take/ai201/.venv/lib/python3.11/site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/lendluy/Documents/MEngg AI/AI 201 2nd Take/ai201/.venv/lib/python3.11/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/lendluy/Documents/MEngg AI/AI 201 2nd Take/ai201/.venv/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lendluy/Documents/MEngg AI/AI 201 2nd Take/ai201/.venv/lib/python3.11/site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "- $X$: Features from `data.csv`\n",
    "- $y$: Labels from `data_labels.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features_file(file_path, dtype=float):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", newline=\"\") as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Read and convert data\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    converted_row = [dtype(val) for val in row]\n",
    "                    data.append(converted_row)\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Error converting value in row: {row}. {str(e)}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Could not find file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading CSV file {file_path}: {str(e)}\")\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "def read_labels_file(file_path, dtype=int):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", newline=\"\") as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Read and convert data\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    value = dtype(row[0])  # Get only first element since there is only 1\n",
    "                    data.append(value)\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Error converting value in row: {row}. {str(e)}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Could not find file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading CSV file {file_path}: {str(e)}\")\n",
    "    \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = read_features_file(\"data.csv\")\n",
    "X_test = read_features_file(\"test_set.csv\")\n",
    "y_raw = read_labels_file(\"data_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels $y_{raw}$ to one-hot vectors\n",
    "Encoding labels into one-hot vectors is proven to be beneficial for classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=8):\n",
    "    # Convert labels to 0-based indexing (subtract 1 since labels start from 1)\n",
    "    zero_based_labels = labels.reshape(-1) - 1\n",
    "    \n",
    "    # Create a zero matrix of shape (n_samples, num_classes)\n",
    "    n_samples = len(labels)\n",
    "    one_hot = np.zeros((n_samples, num_classes))\n",
    "    \n",
    "    # Set the appropriate indices to 1\n",
    "    one_hot[np.arange(n_samples), zero_based_labels] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = one_hot_encode((y_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset\n",
    "Obtain the class distribution and why data augmentation is needed via SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class_distribution(y_one_hot):\n",
    "    # Convert one-hot to class indices (adding 1 since classes are 1-8)\n",
    "    y_classes = np.argmax(y_one_hot, axis=1) + 1\n",
    "    \n",
    "    # Get class frequencies\n",
    "    class_counts = Counter(y_classes)\n",
    "    \n",
    "    # Sort by class labels\n",
    "    sorted_counts = dict(sorted(class_counts.items()))\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_samples = len(y)\n",
    "    percentages = {k: (v/total_samples) for k, v in sorted_counts.items()}\n",
    "    \n",
    "    # Print frequency table\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Class':<10} {'Count':<10} {'Percentage':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for class_label, count in sorted_counts.items():\n",
    "        percentage = percentages[class_label]\n",
    "        print(f\"{class_label:<10} {count:<10} {percentage:>10.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "--------------------------------------------------\n",
      "Class      Count      Percentage\n",
      "--------------------------------------------------\n",
      "1          1625             0.47%\n",
      "2          233              0.07%\n",
      "3          30               0.01%\n",
      "4          483              0.14%\n",
      "5          287              0.08%\n",
      "6          310              0.09%\n",
      "7          52               0.01%\n",
      "8          466              0.13%\n",
      "--------------------------------------------------\n",
      "Total: 3486\n"
     ]
    }
   ],
   "source": [
    "show_class_distribution(np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset by oversampling using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "steps = [(\"o\", smote)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X, y = pipeline.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "--------------------------------------------------\n",
      "Class      Count      Percentage\n",
      "--------------------------------------------------\n",
      "1          1625             0.12%\n",
      "2          1625             0.12%\n",
      "3          1625             0.12%\n",
      "4          1625             0.12%\n",
      "5          1625             0.12%\n",
      "6          1625             0.12%\n",
      "7          1625             0.12%\n",
      "8          1625             0.12%\n",
      "--------------------------------------------------\n",
      "Total: 13000\n"
     ]
    }
   ],
   "source": [
    "show_class_distribution(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the balanced dataset into training and validation sets\n",
    "Split the feature-label pairs $(x,y)$ into training and validation sets such that the validation set contains 800 datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "    \n",
    "    # Create list of indices and shuffle it\n",
    "    indices = list(range(len(X)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    # Split the data\n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "    \n",
    "    X_train = np.array([X[i] for i in train_indices])\n",
    "    X_test = np.array([X[i] for i in test_indices])\n",
    "    y_train = np.array([y[i] for i in train_indices])\n",
    "    y_test = np.array([y[i] for i in test_indices])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=800/len(X), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the balanced dataset as CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(X, y, features_file=\"features.csv\", labels_file=\"labels.csv\"):\n",
    "    try:\n",
    "        # Save features\n",
    "        np.savetxt(features_file, X, delimiter=',')\n",
    "        \n",
    "        # Save labels\n",
    "        np.savetxt(labels_file, y, delimiter=',', fmt='%d')\n",
    "        \n",
    "        # Print info\n",
    "        print(f\"Dataset saved successfully:\")\n",
    "        print(f\"Features shape: {X.shape} -> {features_file}\")\n",
    "        print(f\"Labels shape: {y.shape} -> {labels_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully:\n",
      "Features shape: (12200, 354) -> training_set.csv\n",
      "Labels shape: (12200, 8) -> training_labels.csv\n",
      "Dataset saved successfully:\n",
      "Features shape: (800, 354) -> validation_set.csv\n",
      "Labels shape: (800, 8) -> validation_labels.csv\n"
     ]
    }
   ],
   "source": [
    "save_dataset(X_train, y_train, \"training_set.csv\", \"training_labels.csv\")\n",
    "save_dataset(X_val, y_val, \"validation_set.csv\", \"validation_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
