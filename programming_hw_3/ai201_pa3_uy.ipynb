{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 201 Programming Assignment 3\n",
    "## Multi-Layer Perceptron with Backpropagation\n",
    "\n",
    "Submitted by: \n",
    "Jan Lendl R. Uy, 2019-00312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --quiet numpy matplotlib imblearn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "path_to_train_set_features = \"data.csv\"\n",
    "path_to_train_set_labels = \"data_labels.csv\"\n",
    "path_to_test_set_features = \"test_set.csv\"\n",
    "\n",
    "# Training and validation set filenames\n",
    "filename_train_set_features = \"training_set.csv\"\n",
    "filename_train_set_labels = \"training_labels.csv\"\n",
    "filename_val_set_features = \"validation_set.csv\"\n",
    "filename_val_set_labels = \"validation_labels.csv\"\n",
    "\n",
    "# Filename for saving the trained weights\n",
    "filename_network_weights = \"trained_weights.csv\"\n",
    "\n",
    "# Filenames for predictions of Network A and Network B\n",
    "filename_network_a_pred = \"predictions_for_test_tanh.csv\"\n",
    "filename_network_b_pred = \"predictions_for_test_leakyrelu.csv\"\n",
    "\n",
    "# MLP training parameters and randomization seed\n",
    "EPOCHS = 500\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Reading and Writing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reading and writing CSV files\n",
    "def read_csv(path):\n",
    "    data = []\n",
    "\n",
    "    with open(path, mode=\"r\") as file:\n",
    "        csv_reader = csv.reader(file)  \n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "    return np.array(data, dtype=float)\n",
    "\n",
    "def write_csv(path, data, mode=\"w\"):\n",
    "    # Save features to a CSV file\n",
    "    with open(path, mode=mode, newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "X_imbalanced = read_csv(path_to_train_set_features)\n",
    "Y_imbalanced = read_csv(path_to_train_set_labels)\n",
    "# X_test = read_csv(path_to_test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for one-hot encoding labels\n",
    "def one_hot_encoding(x, length):\n",
    "    encoding = np.zeros(length)\n",
    "    encoding[int(x)-1] = 1\n",
    "    return encoding\n",
    "\n",
    "Y_one_hot_encoded = Y_imbalanced.tolist().copy()\n",
    "for i in range(len(Y_one_hot_encoded)):\n",
    "    Y_one_hot_encoded[i] = one_hot_encoding(Y_one_hot_encoded[i][0], 8)\n",
    "Y_one_hot_encoded = np.array(Y_one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the Dataset via SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance out the existing dataset via SMOTE\n",
    "smote = SMOTE()\n",
    "X_balanced, Y_balanced = smote.fit_resample(X_imbalanced, Y_one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 354)\n",
      "(13000, 8)\n",
      "(12200, 354)\n",
      "(800, 354)\n",
      "(12200, 8)\n",
      "(800, 8)\n"
     ]
    }
   ],
   "source": [
    "def train_val_split(X, Y, val_size, random_state=None):    \n",
    "    # Pair each document with its label\n",
    "    paired = list(zip(X, Y))\n",
    "    \n",
    "    # Shuffle the paired documents and labels\n",
    "    np.random.shuffle(paired)\n",
    "    \n",
    "    # Calculate the number of samples in the validation set\n",
    "    if isinstance(val_size, float):\n",
    "        val_size = int(len(X)-800)\n",
    "    \n",
    "    # Split the paired list into training and validation sets\n",
    "    train_pairs = paired[:-val_size]\n",
    "    val_pairs = paired[-val_size:]\n",
    "    \n",
    "    # Unzip the pairs back into separate lists\n",
    "    train_features, train_labels = zip(*train_pairs)\n",
    "    val_features, val_labels = zip(*val_pairs)\n",
    "    \n",
    "    return np.array(train_features), np.array(val_features), np.array(train_labels), np.array(val_labels)\n",
    "\n",
    "# Retrieve the training, validation, and test sets from the balanced dataset\n",
    "X_train, X_val, Y_train, Y_val = train_val_split(X_balanced, Y_balanced, val_size=800, random_state=0)\n",
    "X_test, Y_test = X_imbalanced, Y_imbalanced\n",
    "\n",
    "print(X_balanced.shape)\n",
    "print(Y_balanced.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and validation sets as CSV files\n",
    "write_csv(filename_train_set_features, X_train)\n",
    "write_csv(filename_train_set_labels, Y_train)\n",
    "write_csv(filename_val_set_features, X_val)\n",
    "write_csv(filename_val_set_labels, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for calculating the model metrics\n",
    "def probabilities_to_class_labels(Y_probs):\n",
    "    return np.argmax(Y_probs, axis=1) + 1\n",
    "\n",
    "def count_misclassifications(Y_true, Y_pred):\n",
    "    misclassifications = 0\n",
    "    for i in range(Y_true.shape[0]):\n",
    "        if Y_true[i] != Y_pred[i]:\n",
    "            misclassifications += 1\n",
    "    \n",
    "    return misclassifications\n",
    "\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    K = len(np.unique(Y_true))  # Number of classes\n",
    "    result = np.zeros((K, K))\n",
    "\n",
    "    for i in range(len(Y_true)):\n",
    "        # Directly use Y_true[i] and Y_pred[i] as indices\n",
    "        result[Y_true[i] - 1, Y_pred[i] - 1] += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def plot_confusion_matrix(Y_true, Y_pred, labels=None):\n",
    "    cm = confusion_matrix(Y_true, Y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # We want to show all ticks and label them with the respective list entries\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate((Y_true, Y_pred)))\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           title='Confusion Matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if cm.dtype == 'float' else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def calculate_model_metrics(Y_true, Y_pred):\n",
    "    # Print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(Y_true, Y_pred)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "    \n",
    "    # Compute the precision, recall, and f1-score per class\n",
    "    precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
    "    recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Compute the macro averages for precision, recall, and f1-score\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    # Compute the Matthews Correlation Coefficient (MCC)\n",
    "    mcc_numerator = np.sum(np.diag(conf_matrix) * np.sum(conf_matrix) - np.sum(conf_matrix, axis=0) * np.sum(conf_matrix, axis=1))\n",
    "    mcc_denominator = np.sqrt(np.sum(conf_matrix) * np.sum(conf_matrix, axis=0) * np.sum(conf_matrix, axis=1) * np.sum(conf_matrix))\n",
    "    mcc = mcc_numerator / mcc_denominator\n",
    "    \n",
    "    return accuracy, macro_precision, macro_recall, macro_f1_score, mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticFunction:\n",
    "    \n",
    "    def __init__(self, a=2.0):\n",
    "        self.a = a\n",
    "        \n",
    "    # Numerically stable version for computing the logistic function\n",
    "    # Uses two versions of the expression depending on the element-wise\n",
    "    # value of x\n",
    "    def forward(self, x):\n",
    "        # Mask for determining which expression to use for logistic function\n",
    "        pos_mask = (x >= 0)\n",
    "        z = np.where(pos_mask, np.exp(-self.a * x), np.exp(self.a * x))\n",
    "        sigma = np.where(pos_mask, 1 / (1 + z), z / (1 + z))\n",
    "        return sigma\n",
    "    \n",
    "    def backward(self, x):\n",
    "        sigma = self.forward(x)\n",
    "        return self.a * sigma * (1 - sigma)\n",
    "    \n",
    "class Tanh:\n",
    "    \n",
    "    def __init__(self, a=1.716, b=2/3):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.a * np.tanh(x * self.b)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return self.a*self.b * (1/np.cosh(self.b*x))**2\n",
    "    \n",
    "class LeakyReLU:\n",
    "    \n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, self.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, \n",
    "                activation_functions=[Tanh(), Tanh(), LogisticFunction()], \n",
    "                learning_rate=0.01,\n",
    "                momentum=0.9, \n",
    "                seed=0,\n",
    "                verbosity=1):\n",
    "        \n",
    "        # Network architecture parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.activation_functions = activation_functions\n",
    "\n",
    "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.depth = len(self.layer_sizes)\n",
    "        # print(f\"layer_sizes = {self.layer_sizes}\")\n",
    "\n",
    "        # Backpropagation parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.weights = self.initialize_weights()\n",
    "        self.best_weights = []\n",
    "        self.velocity = self.initialize_velocity()\n",
    "        \n",
    "        # Flag for determining whether to suppress progress bar during training\n",
    "        self.verbosity = verbosity\n",
    "\n",
    "        # np.random.seed(seed)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "\n",
    "        # Embed the biasing term in the weights\n",
    "        for i in range(self.depth - 1):\n",
    "            w = np.random.randn(self.layer_sizes[i] + 1, self.layer_sizes[i+1]) * np.sqrt(2.0/(self.layer_sizes[i]))\n",
    "            weights.append(w)\n",
    "            # print(w.shape)\n",
    "        return weights\n",
    "\n",
    "    def initialize_velocity(self):\n",
    "        velocity = []\n",
    "        for i in range(self.depth - 1):\n",
    "            velocity_layer = {\n",
    "                \"weights\": np.zeros_like(self.weights[i]),\n",
    "            }\n",
    "            velocity.append(velocity_layer)\n",
    "        return velocity\n",
    "    \n",
    "    def save_best_weights(self, filename=\"trained_weights.csv\"):\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for weight_matrix in self.best_weights:\n",
    "                # Flatten the weight matrix and convert to list\n",
    "                flattened_weights = weight_matrix.flatten().tolist()\n",
    "                # Write the flattened weight matrix to a CSV file\n",
    "                writer.writerow(flattened_weights)\n",
    "\n",
    "    def load_weights(self, filename=\"trained_weights.csv\"):\n",
    "        with open(filename, 'r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            loaded_weights = []\n",
    "            for i, row in enumerate(reader):\n",
    "                # Convert row to floats and reshape according to the weight matrix's expected shape\n",
    "                weight_matrix = np.array(list(map(float, row))).reshape(self.weights[i].shape)\n",
    "                loaded_weights.append(weight_matrix)\n",
    "            self.weights = loaded_weights\n",
    "        \n",
    "    def sum_squared_errors(self, Y_true, Y_pred):\n",
    "        E = 1/2 * np.sum((Y_true - Y_pred)**2)\n",
    "        return E\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.layers = [x]\n",
    "\n",
    "        # Forward pass from input layer to hidden layers\n",
    "        # print(f\"{X.shape} x {self.weights[0].shape} x {self.biases[0].shape}\")\n",
    "        for i in range(self.depth - 1):\n",
    "            bias = np.ones((x.shape[0], 1))\n",
    "            x = np.concatenate((bias, x), axis=1)\n",
    "            z = x @ self.weights[i]\n",
    "            phi = self.activation_functions[i].forward(z)\n",
    "            self.layers.append(phi)\n",
    "            x = phi\n",
    "\n",
    "        return self.layers[-1]\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        # Update weights and biases with momentum\n",
    "        for i in range(self.depth - 1):\n",
    "            # Update the velocity for weights and biases\n",
    "            self.velocity[i][\"weights\"] = self.momentum * self.velocity[i][\"weights\"] - self.learning_rate * self.weight_gradients[i]\n",
    "\n",
    "            # Update the weights and biases using the new velocity\n",
    "            self.weights[i] += self.velocity[i][\"weights\"]\n",
    "\n",
    "    def backpropagation(self, Y_true, Y_pred):\n",
    "        # Initialize gradients\n",
    "        self.weight_gradients = []\n",
    "\n",
    "        # Compute the error\n",
    "        e = Y_true - Y_pred\n",
    "        \n",
    "        # Compute gradients in the output layer\n",
    "        phi_prime = self.activation_functions[-1].backward(self.layers[-1])\n",
    "        delta = -e * phi_prime\n",
    "        weight_gradient = self.layers[-2].T @ delta\n",
    "        bias_gradient = np.sum(delta, axis=0, keepdims=True)\n",
    "        self.weight_gradients.append(np.concatenate((bias_gradient, weight_gradient), axis=0))\n",
    "        \n",
    "        # Compute gradients in the hidden layers\n",
    "        for i in range(-2, -self.depth, -1):\n",
    "            phi_prime = self.activation_functions[i].backward(self.layers[i])\n",
    "            delta = delta @ self.weights[i+1][1:, :].T * phi_prime\n",
    "            weight_gradient = self.layers[i-1].T @ delta\n",
    "            bias_gradient = np.sum(delta, axis=0, keepdims=True)\n",
    "            self.weight_gradients.append(np.concatenate((bias_gradient, weight_gradient), axis=0))\n",
    "            \n",
    "        self.weight_gradients.reverse()\n",
    "        \n",
    "        self.update_parameters()\n",
    "\n",
    "    def generate_mini_batches(self, X, Y, batch_size):\n",
    "        # Yield mini-batches from the dataset \n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            yield X[i:i + batch_size], Y[i:i + batch_size]\n",
    "        \n",
    "    def train(self, X_train, Y_train, X_val, Y_val, epochs, batch_size=DEFAULT_BATCH_SIZE):\n",
    "        # Initialize lists to store training and validation errors\n",
    "        self.training_errors = []\n",
    "        self.validation_errors = []\n",
    "        self.validation_misclassifications = []\n",
    "\n",
    "        lowest_val_error = float(\"inf\")\n",
    "\n",
    "        # Record the start time to profile the runtime of the model training\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Choose the correct iterator based on verbosity\n",
    "        epoch_iterator = tqdm(range(epochs), desc=\"Training Epochs\") if self.verbosity > 0 else range(epochs)\n",
    "\n",
    "        # Perform training for the specified number of epochs\n",
    "        for epoch in epoch_iterator:\n",
    "            # Randomly shuffle the training data at the beginning of each epoch\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train_shuffled = X_train[indices]\n",
    "            Y_train_shuffled = Y_train[indices]\n",
    "\n",
    "            # Train the model with the training data\n",
    "            sum_squared_errors_train = 0\n",
    "            for X_batch, Y_batch in self.generate_mini_batches(X_train_shuffled, Y_train_shuffled, batch_size):\n",
    "                Y_pred = self.forward(X_batch)\n",
    "                self.backpropagation(Y_batch, Y_pred)\n",
    "                sum_squared_errors_train += self.sum_squared_errors(Y_batch, Y_pred)\n",
    "\n",
    "            # Store the cumulative training error over all batches\n",
    "            self.training_errors.append(sum_squared_errors_train)\n",
    "\n",
    "            # Validate the model with the validation data\n",
    "            Y_pred_val = self.forward(X_val)\n",
    "            \n",
    "            sum_squared_errors_val = self.sum_squared_errors(Y_val, Y_pred_val)\n",
    "            \n",
    "            # Calculate and store the validation error and number of \n",
    "            # misclassifications every 5 epochs\n",
    "            if (epoch % 5 == 0):\n",
    "                misclassification = count_misclassifications(probabilities_to_class_labels(Y_val), probabilities_to_class_labels(Y_pred_val))\n",
    "                self.validation_errors.append(sum_squared_errors_val)\n",
    "                self.validation_misclassifications.append(misclassification)\n",
    "            \n",
    "                tqdm.write(f\"Epoch {epoch} | Validation Sum of Squared Errors: {sum_squared_errors_val} | Misclassifications: {misclassification}\")\n",
    "\n",
    "            if sum_squared_errors_val < lowest_val_error:\n",
    "                lowest_val_error = sum_squared_errors_val\n",
    "                self.best_weights = [np.copy(w) for w in self.weights]  # Deep copy of weights\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Model training took {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Find the best set of parameters using grid search that iterates through the following values:\n",
    "- Learning rate\n",
    "- Momentum constant\n",
    "- Hidden layer node count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, Y_train, X_val, Y_val, grid, epochs=EPOCHS, batch_size=DEFAULT_BATCH_SIZE):\n",
    "    best_params = {}\n",
    "    lowest_val_error = float(\"inf\")\n",
    "\n",
    "    total_iterations = len(grid[\"learning_rate\"]) * len(grid[\"momentum\"]) * len(grid[\"hidden_sizes\"])\n",
    "    progress = tqdm(total=total_iterations, desc=\"Hyperparameter Tuning\", position=0)\n",
    "\n",
    "    for learning_rate in grid[\"learning_rate\"]:\n",
    "        for momentum in grid[\"momentum\"]:\n",
    "            for hidden_sizes in grid['hidden_sizes']:\n",
    "                # Initialize the MLP with the current set of hyperparameters\n",
    "                mlp = MultiLayerPerceptron(input_size=X_train.shape[1],\n",
    "                                           hidden_sizes=hidden_sizes,\n",
    "                                           output_size=Y_train.shape[1],\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           momentum=momentum,\n",
    "                                           verbosity=0)\n",
    "\n",
    "                # Train the MLP\n",
    "                mlp.train(X_train, Y_train, X_val, Y_val, epochs=epochs, batch_size=batch_size)\n",
    "                \n",
    "                # Evaluate the MLP based on validation error at the end of training\n",
    "                val_error = mlp.validation_errors[-1]  # Assuming validation errors are stored after each epoch\n",
    "                \n",
    "                # Update the best parameters if current model is better\n",
    "                if val_error < lowest_val_error:\n",
    "                    lowest_val_error = val_error\n",
    "                    best_params = {\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'momentum': momentum,\n",
    "                        'hidden_sizes': hidden_sizes,\n",
    "                        'validation_error': val_error\n",
    "                    }\n",
    "                    print(f\"New best parameters found: {best_params}\")\n",
    "\n",
    "                progress.update(1)  # Increment the progress after each set of parameters is evaluated\n",
    "\n",
    "    progress.close()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee2daa68d2d4c4caf0f9bf39babf5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hyperparameter Tuning:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Validation Sum of Squared Errors: 183.34313091332598 | Misclassifications: 233\n",
      "Epoch 5 | Validation Sum of Squared Errors: 61.78664636162836 | Misclassifications: 54\n",
      "Epoch 10 | Validation Sum of Squared Errors: 34.56039019604218 | Misclassifications: 34\n",
      "Epoch 15 | Validation Sum of Squared Errors: 31.37582620767607 | Misclassifications: 31\n",
      "Epoch 20 | Validation Sum of Squared Errors: 40.852765563488134 | Misclassifications: 47\n",
      "Epoch 25 | Validation Sum of Squared Errors: 17.317912212919524 | Misclassifications: 17\n",
      "Epoch 30 | Validation Sum of Squared Errors: 16.049843707233975 | Misclassifications: 16\n",
      "Epoch 35 | Validation Sum of Squared Errors: 10.237352701401996 | Misclassifications: 11\n",
      "Epoch 40 | Validation Sum of Squared Errors: 14.04789758355813 | Misclassifications: 14\n",
      "Epoch 45 | Validation Sum of Squared Errors: 13.055082748923589 | Misclassifications: 14\n",
      "Epoch 50 | Validation Sum of Squared Errors: 13.114438786206101 | Misclassifications: 15\n",
      "Epoch 55 | Validation Sum of Squared Errors: 10.568547867833598 | Misclassifications: 10\n",
      "Epoch 60 | Validation Sum of Squared Errors: 8.385195448057408 | Misclassifications: 7\n",
      "Epoch 65 | Validation Sum of Squared Errors: 10.0736185298084 | Misclassifications: 7\n",
      "Epoch 70 | Validation Sum of Squared Errors: 12.304036594240019 | Misclassifications: 9\n",
      "Epoch 75 | Validation Sum of Squared Errors: 21.902996531801666 | Misclassifications: 21\n",
      "Epoch 80 | Validation Sum of Squared Errors: 19.390315156714834 | Misclassifications: 22\n",
      "Epoch 85 | Validation Sum of Squared Errors: 8.677502200802031 | Misclassifications: 8\n",
      "Epoch 90 | Validation Sum of Squared Errors: 11.320064641421002 | Misclassifications: 9\n",
      "Epoch 95 | Validation Sum of Squared Errors: 7.548799456128088 | Misclassifications: 6\n",
      "Model training took 5.549237012863159 seconds\n",
      "New best parameters found: {'learning_rate': 0.001, 'momentum': 0.9, 'hidden_sizes': [10, 10], 'validation_error': 7.548799456128088}\n",
      "Epoch 0 | Validation Sum of Squared Errors: 125.4913273702251 | Misclassifications: 147\n",
      "Epoch 5 | Validation Sum of Squared Errors: 58.00342827174258 | Misclassifications: 47\n",
      "Epoch 10 | Validation Sum of Squared Errors: 40.19723043956601 | Misclassifications: 32\n",
      "Epoch 15 | Validation Sum of Squared Errors: 22.869361148997264 | Misclassifications: 19\n",
      "Epoch 20 | Validation Sum of Squared Errors: 14.54686673937946 | Misclassifications: 14\n",
      "Epoch 25 | Validation Sum of Squared Errors: 14.434309055471537 | Misclassifications: 17\n",
      "Epoch 30 | Validation Sum of Squared Errors: 32.250546590937745 | Misclassifications: 33\n",
      "Epoch 35 | Validation Sum of Squared Errors: 12.523975066263171 | Misclassifications: 15\n",
      "Epoch 40 | Validation Sum of Squared Errors: 10.378259798847242 | Misclassifications: 13\n",
      "Epoch 45 | Validation Sum of Squared Errors: 10.04872111836973 | Misclassifications: 8\n",
      "Epoch 50 | Validation Sum of Squared Errors: 7.971531820668749 | Misclassifications: 9\n",
      "Epoch 55 | Validation Sum of Squared Errors: 5.927817741604313 | Misclassifications: 5\n",
      "Epoch 60 | Validation Sum of Squared Errors: 11.424424943910303 | Misclassifications: 14\n",
      "Epoch 65 | Validation Sum of Squared Errors: 4.823037229831657 | Misclassifications: 5\n",
      "Epoch 70 | Validation Sum of Squared Errors: 6.181561351863079 | Misclassifications: 5\n",
      "Epoch 75 | Validation Sum of Squared Errors: 6.687744259525311 | Misclassifications: 8\n",
      "Epoch 80 | Validation Sum of Squared Errors: 11.108757978583727 | Misclassifications: 12\n",
      "Epoch 85 | Validation Sum of Squared Errors: 3.4762509825214174 | Misclassifications: 2\n",
      "Epoch 90 | Validation Sum of Squared Errors: 4.371833129624975 | Misclassifications: 3\n",
      "Epoch 95 | Validation Sum of Squared Errors: 4.346955981312231 | Misclassifications: 3\n",
      "Model training took 15.804942846298218 seconds\n",
      "New best parameters found: {'learning_rate': 0.001, 'momentum': 0.9, 'hidden_sizes': [50, 50], 'validation_error': 4.346955981312231}\n",
      "Epoch 0 | Validation Sum of Squared Errors: 155.8199509929562 | Misclassifications: 188\n",
      "Epoch 5 | Validation Sum of Squared Errors: 54.47764193737028 | Misclassifications: 52\n",
      "Epoch 10 | Validation Sum of Squared Errors: 34.29340250750866 | Misclassifications: 37\n",
      "Epoch 15 | Validation Sum of Squared Errors: 20.377859398897257 | Misclassifications: 16\n",
      "Epoch 20 | Validation Sum of Squared Errors: 25.84730618031203 | Misclassifications: 27\n",
      "Epoch 25 | Validation Sum of Squared Errors: 14.255351440695271 | Misclassifications: 14\n",
      "Epoch 30 | Validation Sum of Squared Errors: 18.093080723398757 | Misclassifications: 18\n",
      "Epoch 35 | Validation Sum of Squared Errors: 10.107775311823524 | Misclassifications: 10\n",
      "Epoch 40 | Validation Sum of Squared Errors: 10.831803691397722 | Misclassifications: 13\n",
      "Epoch 45 | Validation Sum of Squared Errors: 17.60548532558135 | Misclassifications: 18\n",
      "Epoch 50 | Validation Sum of Squared Errors: 9.253203202792122 | Misclassifications: 10\n",
      "Epoch 55 | Validation Sum of Squared Errors: 9.064871391376183 | Misclassifications: 10\n",
      "Epoch 60 | Validation Sum of Squared Errors: 10.798529798791439 | Misclassifications: 8\n",
      "Epoch 65 | Validation Sum of Squared Errors: 13.804550689881928 | Misclassifications: 18\n",
      "Epoch 70 | Validation Sum of Squared Errors: 7.024875686317877 | Misclassifications: 9\n",
      "Epoch 75 | Validation Sum of Squared Errors: 6.2993590913974735 | Misclassifications: 9\n",
      "Epoch 80 | Validation Sum of Squared Errors: 24.31007374942292 | Misclassifications: 30\n",
      "Epoch 85 | Validation Sum of Squared Errors: 5.794326893393735 | Misclassifications: 7\n",
      "Epoch 90 | Validation Sum of Squared Errors: 7.526209364090253 | Misclassifications: 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the grid of hyperparameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.99\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m], [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m], [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m]]  \u001b[38;5;66;03m# Different configurations of hidden layers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 8\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(X_train, Y_train, X_val, Y_val, grid, epochs, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MultiLayerPerceptron(input_size\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     13\u001b[0m                            hidden_sizes\u001b[38;5;241m=\u001b[39mhidden_sizes,\n\u001b[1;32m     14\u001b[0m                            output_size\u001b[38;5;241m=\u001b[39mY_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     15\u001b[0m                            learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     16\u001b[0m                            momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m     17\u001b[0m                            verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the MLP\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate the MLP based on validation error at the end of training\u001b[39;00m\n\u001b[1;32m     23\u001b[0m val_error \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mvalidation_errors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming validation errors are stored after each epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 157\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.train\u001b[0;34m(self, X_train, Y_train, X_val, Y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_mini_batches(X_train_shuffled, Y_train_shuffled, batch_size):\n\u001b[1;32m    156\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     sum_squared_errors_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_squared_errors(Y_batch, Y_pred)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Store the cumulative training error over all batches\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 119\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.backpropagation\u001b[0;34m(self, Y_true, Y_pred)\u001b[0m\n\u001b[1;32m    117\u001b[0m     delta \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m phi_prime\n\u001b[1;32m    118\u001b[0m     weight_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m delta\n\u001b[0;32m--> 119\u001b[0m     bias_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_gradients\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate((bias_gradient, weight_gradient), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_gradients\u001b[38;5;241m.\u001b[39mreverse()\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"momentum\": [0.9, 0.95, 0.99],\n",
    "    \"hidden_sizes\": [[10, 10], [50, 50], [100, 100]]  # Different configurations of hidden layers\n",
    "}\n",
    "\n",
    "best_params = grid_search(X_train, Y_train, X_val, Y_val, param_grid, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network A\n",
    "\n",
    "Activation Functions\n",
    "1. Hyperbolic Tangent $\\tanh$ (a = 1.716, b = 2/3)\n",
    "2. Hyperbolic Tangent $\\tanh$ (a = 1.716, b = 2/3)\n",
    "3. Logistic Function (a = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP with tanh as the squashing function\n",
    "# for the hidden layers, default parameters\n",
    "input_size, output_size = X_train.shape[1], Y_train.shape[1]\n",
    "hidden_layer_sizes = [100, 100]\n",
    "activation_functions = [Tanh(), \n",
    "                        Tanh(), \n",
    "                        LogisticFunction()\n",
    "                        ]\n",
    "                        \n",
    "\n",
    "# Train the MLP with the training and validation sets\n",
    "mlp_a = MultiLayerPerceptron(input_size=input_size, \n",
    "                             hidden_sizes=hidden_layer_sizes, \n",
    "                             output_size=output_size, \n",
    "                             activation_functions=activation_functions,\n",
    "                             learning_rate=0.005)\n",
    "mlp_a.train(X_train, Y_train, X_val, Y_val, EPOCHS, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained weights of Network A for inference and performance evaluation\n",
    "mlp_a.save_best_weights(filename_network_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_flattened = Y_test.flatten().astype(np.int64)\n",
    "mlp_a.load_weights(filename_network_weights)\n",
    "Y_pred_network_a = mlp_a.forward(X_test)\n",
    "Y_pred_labels = probabilities_to_class_labels(Y_pred_network_a)\n",
    "\n",
    "cm = confusion_matrix(Y_test_flattened, Y_pred_labels)\n",
    "accuracy, macro_precision, macro_recall, macro_f1_score, mcc = calculate_model_metrics(Y_test_flattened, Y_pred_labels)\n",
    "\n",
    "print(f\"MLP Network A Confusion Matrix:\")\n",
    "plot_confusion_matrix(Y_test_flattened, Y_pred_labels)\n",
    "print(f\"\\nMLP Network A Accuracy: {accuracy}\")\n",
    "print(f\"MLP Network A Precision: {macro_precision}\")\n",
    "print(f\"MLP Network A Recall: {macro_recall}\")\n",
    "print(f\"MLP Network A F1-Score: {macro_f1_score}\")\n",
    "print(f\"MLP Network A Matthew's Coefficient Correlation: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = np.linspace(0, EPOCHS + 1, EPOCHS)\n",
    "validation_epochs = np.linspace(0, EPOCHS + 1, EPOCHS//5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))  # Create 1 row, 2 columns of subplots\n",
    "\n",
    "# Training Error Plot\n",
    "axes[0].plot(training_epochs, mlp_a.training_errors, label=\"Training Error\", \n",
    "             marker=\"o\", linestyle=\"-\", color='blue')\n",
    "axes[0].set_title(\"Network A Training Error vs. Epoch\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Sum of Squared Errors\")\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Validation Error Plot\n",
    "axes[1].plot(validation_epochs, mlp_a.validation_errors, label=\"Validation Error\", \n",
    "             marker=\"s\", linestyle='--', color='orange')\n",
    "axes[1].set_title(\"Network A Validation Error vs. Epoch\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Sum of Squared Errors\")\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network B\n",
    "\n",
    "Activation Functions\n",
    "1. Leaky ReLU $\\tanh$ (alpha = 0.01)\n",
    "2. Leaky ReLU $\\tanh$ (alpha = 0.01)\n",
    "3. Logistic Function (a = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP with the Leaky ReLU as the squashing function\n",
    "# for the hidden layers\n",
    "input_size, output_size = X_train.shape[1], Y_train.shape[1]\n",
    "hidden_layer_sizes = [100, 100]\n",
    "activation_functions = [LeakyReLU(), \n",
    "                        LeakyReLU(), \n",
    "                        LogisticFunction()\n",
    "                        ]\n",
    "\n",
    "# Train the MLP with the training and validation sets\n",
    "mlp_b = MultiLayerPerceptron(input_size=input_size, hidden_sizes=hidden_layer_sizes, output_size=output_size, activation_functions=activation_functions)\n",
    "mlp_b.train(X_train, Y_train, X_val, Y_val, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained weights of Network B for inference and performance evaluation\n",
    "mlp_b.save_best_weights(filename_network_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_flattened = Y_test.flatten().astype(np.int64)\n",
    "mlp_b.load_weights(filename_network_weights)\n",
    "Y_pred_network_b = mlp_b.forward(X_test)\n",
    "Y_pred_labels = probabilities_to_class_labels(Y_pred_network_b)\n",
    "\n",
    "cm = confusion_matrix(Y_test_flattened, Y_pred_labels)\n",
    "accuracy, macro_precision, macro_recall, macro_f1_score, mcc = calculate_model_metrics(Y_test_flattened, Y_pred_labels)\n",
    "\n",
    "print(f\"MLP Network B Confusion Matrix:\")\n",
    "plot_confusion_matrix(Y_test_flattened, Y_pred_labels)\n",
    "print(f\"\\nMLP Network B Accuracy: {accuracy}\")\n",
    "print(f\"MLP Network B Accuracy: {accuracy}\")\n",
    "print(f\"MLP Network B Precision: {macro_precision}\")\n",
    "print(f\"MLP Network B Recall: {macro_recall}\")\n",
    "print(f\"MLP Network B F1-Score: {macro_f1_score}\")\n",
    "print(f\"MLP Network B Matthew's Coefficient Correlation: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = np.linspace(0, EPOCHS + 1, EPOCHS)\n",
    "validation_epochs = np.linspace(0, EPOCHS + 1, EPOCHS//5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))  # Create 1 row, 2 columns of subplots\n",
    "\n",
    "# Training Error Plot\n",
    "axes[0].plot(training_epochs, mlp_b.training_errors, label=\"Training Error\", \n",
    "             marker=\"o\", linestyle=\"-\", color='blue')\n",
    "axes[0].set_title(\"Network B Training Error vs. Epoch\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Sum of Squared Errors\")\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Validation Error Plot\n",
    "axes[1].plot(validation_epochs, mlp_b.validation_errors, label=\"Validation Error\", \n",
    "             marker=\"s\", linestyle='--', color='orange')\n",
    "axes[1].set_title(\"Network B Validation Error vs. Epoch\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Sum of Squared Errors\")\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions of Network A\n",
    "write_csv(filename_network_a_pred, Y_pred_network_a)\n",
    "\n",
    "# Save the predictions of Network B\n",
    "write_csv(filename_network_b_pred, Y_pred_network_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
