{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 201 Programming Assignment 3\n",
    "## Multi-Layer Perceptron with Backpropagation\n",
    "\n",
    "Submitted by: \n",
    "Jan Lendl R. Uy, 2019-00312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "path_to_train_set_features = \"data.csv\"\n",
    "path_to_train_set_labels = \"data_labels.csv\"\n",
    "path_to_test_set_features = \"test_set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    data = []\n",
    "\n",
    "    with open(path, mode=\"r\") as file:\n",
    "        csv_reader = csv.reader(file)  \n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "    return np.array(data, dtype=float)\n",
    "\n",
    "X = read_csv(path_to_train_set_features)\n",
    "Y = read_csv(path_to_train_set_labels)\n",
    "# X_test = read_csv(path_to_test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, length):\n",
    "    encoding = np.zeros(length)\n",
    "    encoding[int(x)-1] = 1\n",
    "    return encoding\n",
    "\n",
    "Y = Y.tolist()\n",
    "for i in range(len(Y)):\n",
    "    Y[i] = one_hot_encoding(Y[i][0], 8)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 354)\n",
      "(13000, 8)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE()\n",
    "X, Y = smote.fit_resample(X, Y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12200, 354)\n",
      "(800, 354)\n",
      "(12200, 8)\n",
      "(800, 8)\n"
     ]
    }
   ],
   "source": [
    "def custom_train_test_split(X, Y, val_size, random_state=None):    \n",
    "    # Pair each document with its label\n",
    "    paired = list(zip(X, Y))\n",
    "    \n",
    "    # Shuffle the paired documents and labels\n",
    "    np.random.shuffle(paired)\n",
    "    \n",
    "    # Calculate the number of samples in the test set\n",
    "    if isinstance(val_size, float):\n",
    "        val_size = int(len(X)-800)\n",
    "    \n",
    "    # Split the paired list into training and testing sets\n",
    "    train_pairs = paired[:-val_size]\n",
    "    val_pairs = paired[-val_size:]\n",
    "    \n",
    "    # Unzip the pairs back into separate lists\n",
    "    train_features, train_labels = zip(*train_pairs)\n",
    "    val_features, val_labels = zip(*val_pairs)\n",
    "    \n",
    "    return np.array(train_features), np.array(val_features), np.array(train_labels), np.array(val_labels)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = custom_train_test_split(X, Y, val_size=800, random_state=62)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, \n",
    "                activation_functions=[\"tanh\", \"tanh\", \"logistic\"], \n",
    "                batch_size=8, \n",
    "                learning_rate=0.01,\n",
    "                momentum=0.9, \n",
    "                seed=0):\n",
    "        \n",
    "        # Network architecture parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.activation_functions = activation_functions\n",
    "\n",
    "        self.layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.depth = len(self.layer_sizes)\n",
    "        print(f\"layer_sizes = {self.layer_sizes}\")\n",
    "\n",
    "        # Backpropagation parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.layers = []\n",
    "        self.weights = self.__initialize_weights()\n",
    "        self.biases = self.__initialize_biases()\n",
    "        self.velocity = self.__initialize_velocity()\n",
    "        \n",
    "        # Gradients of each layer to be used for learning\n",
    "        self.weight_gradients = []\n",
    "        self.bias_gradients = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def __initialize_weights(self):\n",
    "        weights = []\n",
    "        # print(f\"Weights shapes\")\n",
    "        for i in range(self.depth - 1):\n",
    "            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(2.0/(self.layer_sizes[i]))\n",
    "            weights.append(w)\n",
    "            # print(w.shape)\n",
    "        return weights\n",
    "    \n",
    "    def __initialize_biases(self):\n",
    "        biases = []\n",
    "        # print(f\"Biases shapes\")\n",
    "        for i in range(self.depth - 1):\n",
    "            b = np.zeros((1, self.layer_sizes[i+1]))\n",
    "            biases.append(b)\n",
    "            # print(b.shape)\n",
    "        return biases\n",
    "\n",
    "    def __initialize_velocity(self):\n",
    "        velocity = []\n",
    "        for i in range(self.depth - 1):\n",
    "            velocity_layer = {\n",
    "                \"weights\": np.zeros_like(self.weights[i]),\n",
    "                \"biases\": np.zeros_like(self.biases[i])\n",
    "            }\n",
    "            velocity.append(velocity_layer)\n",
    "        return velocity\n",
    "    \n",
    "    def logistic(self, x, a=2.0):\n",
    "        return 1 / (1 + np.exp(-a * x))\n",
    "\n",
    "    def logistic_derivative(self, x, a=2.0):\n",
    "        sigma = self.logistic(x)\n",
    "        return a * sigma * (1 - sigma)\n",
    "    \n",
    "    def tanh(self, x, a=1.716, b=2/3):\n",
    "        return a * np.tanh(x * b)\n",
    "\n",
    "    def tanh_derivative(self, x, a=1.716, b=2/3):\n",
    "        return a*b * (1/np.cosh(b*x))**2\n",
    "    \n",
    "    def leaky_relu(x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "    def leaky_relu_derivative(x, alpha=0.01):\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    def activation(self, X, function):\n",
    "        \n",
    "        if function == \"logistic\":\n",
    "            return self.logistic(X)\n",
    "        elif function == \"tanh\":\n",
    "            return self.tanh(X)\n",
    "        elif function == \"leaky_relu\":\n",
    "            return self.leaky_relu(X)\n",
    "        else:\n",
    "            raise ValueError(f\"{function} is an unsupported activation function!\")\n",
    "        \n",
    "    def activation_derivative(self, X, function):\n",
    "        \n",
    "        if function == \"logistic\":\n",
    "            return self.logistic_derivative(X)\n",
    "        elif function == \"tanh\":\n",
    "            return self.tanh_derivative(X)\n",
    "        elif function == \"leaky_relu\":\n",
    "            return self.leaky_relu_derivative(X)\n",
    "        else:\n",
    "            raise ValueError(f\"{function} is an unsupported activation function!\")\n",
    "    \n",
    "    def sum_squared_errors(self, Y_true, Y_pred):\n",
    "        \n",
    "        E = 1/2 * np.sum((Y_true - Y_pred)**2)\n",
    "        return E\n",
    "    \n",
    "    def __forward(self, x):\n",
    "\n",
    "        # Insert the input as the first layer of the MLP\n",
    "        self.layers.append(x)\n",
    "        \n",
    "        # Forward pass from input layer to output layer\n",
    "        # print(f\"{X.shape} x {self.weights[0].shape} x {self.biases[0].shape}\")\n",
    "        for i in range(self.depth - 1):\n",
    "            z = x @ self.weights[i] + self.biases[i]\n",
    "            phi = self.activation(z, self.activation_functions[i])\n",
    "            self.layers.append(phi)\n",
    "            x = phi\n",
    "\n",
    "        return self.layers[-1]\n",
    "    \n",
    "    def __update_parameters(self):\n",
    "        # Update weights and biases with momentum\n",
    "        for i in range(self.depth - 1):\n",
    "            # Update the velocity for weights and biases\n",
    "            self.velocity[i][\"weights\"] = self.momentum * self.velocity[i][\"weights\"] - self.learning_rate * self.weight_gradients[i]\n",
    "            self.velocity[i][\"biases\"] = self.momentum * self.velocity[i][\"biases\"] - self.learning_rate * self.bias_gradients[i]\n",
    "\n",
    "            # Update the weights and biases using the new velocity\n",
    "            self.weights[i] += self.velocity[i]['weights']\n",
    "            self.biases[i] += self.velocity[i]['biases']\n",
    "\n",
    "    def __backpropagation(self, Y_true):\n",
    "\n",
    "        # Compute the sum of errors\n",
    "        Y_pred = self.layers[-1]\n",
    "        e = Y_true - Y_pred\n",
    "        \n",
    "        # Compute gradients in the output layer\n",
    "        phi_prime = self.activation_derivative(self.layers[-1], self.activation_functions[-1])\n",
    "        delta = -e * phi_prime\n",
    "        weight_gradient = self.layers[-2].T @ delta\n",
    "        bias_gradient = np.sum(delta, axis=0, keepdims=True)\n",
    "        self.weight_gradients.append(weight_gradient)\n",
    "        self.bias_gradients.append(bias_gradient)\n",
    "        \n",
    "        # Compute gradients in the hidden layers\n",
    "        for i in range(-2, -self.depth, -1):\n",
    "            phi_prime = self.activation_derivative(self.layers[i], self.activation_functions[i])\n",
    "            delta = delta @ self.weights[i+1].T * phi_prime\n",
    "            weight_gradient = self.layers[i-1].T @ delta\n",
    "            bias_gradient = np.sum(delta, axis=0, keepdims=True)\n",
    "            self.weight_gradients.append(weight_gradient)\n",
    "            self.bias_gradients.append(bias_gradient)\n",
    "            \n",
    "        self.weight_gradients.reverse()\n",
    "        self.bias_gradients.reverse()\n",
    "        \n",
    "        self.__update_parameters()\n",
    "        \n",
    "    def train(self, X_train, Y_train, epochs):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            Y_pred = self.__forward(X_train)\n",
    "            self.__backpropagation(Y_train)\n",
    "            if (i % 100 == 0):\n",
    "                print(f\"Epoch {i}, Sum of Squared Errors: {self.sum_squared_errors(Y_train, Y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_sizes = [354, 100, 100, 8]\n",
      "Epoch 0, Sum of Squared Errors: 13990.63921920437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cq/5p30h6x5741g3nvfdq8t32g40000gn/T/ipykernel_11961/1966514616.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a * x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Sum of Squared Errors: 14533.930007921286\n",
      "Epoch 200, Sum of Squared Errors: 5275.29250658966\n",
      "Epoch 300, Sum of Squared Errors: 2471.6040415599364\n",
      "Epoch 400, Sum of Squared Errors: 3313.718653901646\n",
      "Epoch 500, Sum of Squared Errors: 2787.490310970903\n",
      "Epoch 600, Sum of Squared Errors: 1623.478027207096\n",
      "Epoch 700, Sum of Squared Errors: 1961.5917019722701\n",
      "Epoch 800, Sum of Squared Errors: 2034.537117706747\n",
      "Epoch 900, Sum of Squared Errors: 2081.12965243935\n",
      "Epoch 1000, Sum of Squared Errors: 1908.9296299654006\n",
      "Epoch 1100, Sum of Squared Errors: 2082.429257670056\n",
      "Epoch 1200, Sum of Squared Errors: 1552.260509444332\n",
      "Epoch 1300, Sum of Squared Errors: 1493.0697426208317\n",
      "Epoch 1400, Sum of Squared Errors: 1438.7337914690177\n",
      "Epoch 1500, Sum of Squared Errors: 1392.9464657462968\n",
      "Epoch 1600, Sum of Squared Errors: 1854.658767315432\n",
      "Epoch 1700, Sum of Squared Errors: 2002.2088053956916\n",
      "Epoch 1800, Sum of Squared Errors: 2647.9278931647864\n",
      "Epoch 1900, Sum of Squared Errors: 2299.485681083451\n",
      "Epoch 2000, Sum of Squared Errors: 1740.2162873726866\n",
      "Epoch 2100, Sum of Squared Errors: 1281.694500764119\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5000\n",
    "input_size, output_size = X_train.shape[1], Y_train.shape[1]\n",
    "hidden_layer_sizes = [100, 100]\n",
    "activation_functions = [\"tanh\", \"tanh\", \"logistic\"]\n",
    "\n",
    "mlp = MultiLayerPerceptron(input_size=input_size, hidden_sizes=hidden_layer_sizes, output_size=output_size, activation_functions=activation_functions)\n",
    "mlp.train(X_train, Y_train, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
